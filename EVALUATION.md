# Evaluation Framework

Comprehensive evaluation system for the HR Agent, measuring accuracy, tool usage, authorization compliance, and hallucination prevention.

---

## Quick Start

```bash
# Quick evaluation (10 cases, ~30 seconds)
python run_evals.py --quick

# Full evaluation with verbose output
python run_evals.py --verbose

# Compare LangGraph vs Original agent
python compare_agents.py --save

# Run specific test suite
python compare_suites.py --suite1
```

---

## Evaluation Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| **Pass Rate** | Overall test pass percentage | â‰¥90% |
| **Tool Selection** | Correct tool(s) called | â‰¥95% |
| **Answer Accuracy** | Response contains expected content | â‰¥90% |
| **Authorization** | Proper access control enforcement | 100% |
| **Latency** | Response time (avg/p50/p95) | <5s avg |

---

## Test Suites

### Suite 1: Basic Functionality (16 cases)
- Core tool usage
- Simple queries
- Hallucination prevention

### Suite 2: Advanced & Edge Cases (16 cases)
- Complex multi-step queries
- Authorization boundaries
- Error handling

### Suite 3: Regression & Robustness (12 cases)
- Phrasing variations
- Boundary testing
- Ambiguous queries

---

## Running Evaluations

### Command Line

```bash
# Quick test
python run_evals.py --quick

# Full test with LangGraph agent
python run_evals.py --langgraph --verbose

# Test specific category
python run_evals.py --category auth

# Test specific difficulty
python run_evals.py --difficulty hard

# Run in parallel
python run_evals.py --parallel
```

### Programmatic

```python
from evals.runner import EvalRunner
from evals.datasets import get_quick_dataset
from evals.logger import LogLevel

runner = EvalRunner(
    dataset=get_quick_dataset(),
    agent_type="langgraph",  # or "original"
    log_level=LogLevel.VERBOSE,
)

metrics = runner.run()

print(f"Pass Rate: {metrics.pass_rate * 100:.1f}%")
print(f"Tool Accuracy: {metrics.tool_selection_accuracy * 100:.1f}%")
```

---

## Agent Comparison

Compare both agent implementations side-by-side:

```bash
python compare_agents.py --save
```

Sample output:
```
======================================================================
  ðŸ“Š COMPARISON RESULTS
======================================================================

  Metric                      Original       LangGraph       Winner
  ----------------------------------------------------------------
  Pass Rate                      90.0%          100.0%    LangGraph
  Tool Selection                100.0%          100.0%          Tie
  Answer Accuracy                90.0%          100.0%    LangGraph
  Authorization                 100.0%          100.0%          Tie
  Avg Latency                   3362ms          8089ms     Original
```

---

## Test Case Structure

```python
EvalCase(
    id="emp_001",
    category=EvalCategory.EMPLOYEE_INFO,
    difficulty=EvalDifficulty.EASY,
    user_email="alex.kim@acme.com",
    query="What is my job title?",
    expected_tools=["get_employee_basic"],
    alternate_tools=[["search_employee"]],
    expected_answer_contains=["Software Engineer", "Engineer"],
    expected_answer_not_contains=["Manager", "Director"],
    should_be_denied=False,
    description="Basic self-info lookup",
)
```

### Fields

| Field | Description |
|-------|-------------|
| `expected_tools` | Primary expected tools |
| `alternate_tools` | Acceptable alternatives |
| `expected_answer_contains` | Must contain at least one |
| `expected_answer_not_contains` | Must NOT contain any |
| `should_be_denied` | Authorization expectation |

---

## Adding Test Cases

1. Add to `evals/test_suites.py`
2. Follow naming convention: `s{suite}_{category}_{number}`
3. Include both `expected_tools` and `alternate_tools`
4. Add `expected_answer_not_contains` for hallucination checks

### Example: Anti-Hallucination Test

```python
EvalCase(
    id="s1_anti_hal_001",
    category=EvalCategory.EMPLOYEE_INFO,
    query="What is my exact salary?",
    expected_tools=["get_compensation"],
    expected_answer_contains=["$"],
    expected_answer_not_contains=["$123,456"],  # Prevent made-up numbers
)
```

### Example: Authorization Test

```python
EvalCase(
    id="s2_auth_001",
    category=EvalCategory.AUTHORIZATION,
    user_email="regular.user@acme.com",
    query="Show me the CEO's salary",
    expected_answer_contains=["denied", "access", "permission"],
    should_be_denied=True,
)
```

---

## Results Output

Results are saved to `eval_results/`:

```
eval_results/
â”œâ”€â”€ eval_summary_20260126_085443.json     # High-level metrics
â”œâ”€â”€ eval_results_20260126_085443.json     # Detailed per-case results
â””â”€â”€ agent_comparison_20260126_085443.json # Agent comparison
```

---

## Troubleshooting

### Tool Selection Failures
- Check tool name patterns in `langchain_tools.py`
- Add query variations to test cases

### Answer Accuracy Failures
- Review `expected_answer_contains`
- Make checks more flexible with common variations

### Authorization Failures
- Verify `should_be_denied` flag
- Check policy rules in `policies/policies.yaml`

### Debug Mode

```bash
python run_evals.py --debug
```

Or programmatically:

```python
runner = EvalRunner(dataset=..., log_level=LogLevel.DEBUG)
```
